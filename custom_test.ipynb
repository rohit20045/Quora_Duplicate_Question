{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from fuzzywuzzy import fuzz\n",
    "import xgboost as xgb\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('xgboost_model1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = {\n",
    "    # Punctuation\n",
    "    '.', ',', '!', '?', ';', ':', '\"', \"'\", '-', '...', '(', ')', '[', ']', '{', '}',\n",
    "\n",
    "    # Prepositions\n",
    "    'in', 'on', 'at',\n",
    "    'of', 'to', 'for', 'with',\n",
    "    'by', 'from', 'about',\n",
    "    'into', 'onto', 'upon',\n",
    "    'within', 'without',\n",
    "\n",
    "    # Articles\n",
    "    'a', 'an', 'the',\n",
    "\n",
    "    # Conjunctions\n",
    "    'and', 'or', 'but',\n",
    "    'so', 'yet', 'nor',\n",
    "    'for',\n",
    "\n",
    "    # Helping Verbs\n",
    "    'am', 'is', 'are',\n",
    "    'was', 'were',\n",
    "    'be', 'been',\n",
    "    'have', 'has', 'had',\n",
    "    'do', 'does', 'did',\n",
    "    'can', 'could',\n",
    "    'will', 'would',\n",
    "    'shall', 'should',\n",
    "    'may', 'might',\n",
    "    'must',\n",
    "\n",
    "    # Common Pronouns\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "    'me', 'him', 'her', 'us', 'them',\n",
    "    'my', 'your', 'his', 'her', 'its', 'our', 'their',\n",
    "    'mine', 'yours', 'hers', 'ours', 'theirs',\n",
    "    'this', 'that', 'these', 'those',\n",
    "\n",
    "    # Common Adverbs\n",
    "    'now', 'then',\n",
    "    'here', 'there',\n",
    "    'quickly', 'slowly',\n",
    "    'very', 'too',\n",
    "    \n",
    "    # Quantifier\n",
    "    \"all\", \"some\", \"any\", \"no\", \"few\", \"many\", \"several\", \"much\", \"most\", \n",
    "    \"more\", \"less\", \"little\", \"a lot of\", \"a few\", \"a little\", \"enough\", \n",
    "    \"lots of\", \"plenty of\", \"each\", \"every\", \"either\", \"neither\", \"both\", \"none\"\n",
    "}\n",
    "\n",
    "interogative_words = {\"what\", \"how\", \"why\", \"when\", \"where\", \"which\", \"who\", \"whom\", \"whose\"}\n",
    "\n",
    "extra_intero_words ={\"are\", \"is\", \"am\", \"was\", \"were\", \"do\", \"does\", \"did\", \"can\", \"could\",\n",
    "                      \"will\", \"would\", \"shall\", \"should\", \"have\", \"has\", \"had\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_word_out(text1,text2,unique_word_set):\n",
    "    words = re.findall(r'\\b\\w+\\b', text1.lower())\n",
    "    \n",
    "    uni_text1 = \"\"\n",
    "    for word in words:\n",
    "        if word in unique_word_set:\n",
    "            uni_text1+=word+' '\n",
    "    \n",
    "    words = re.findall(r'\\b\\w+\\b', text2.lower())\n",
    "    uni_text2 = \"\"\n",
    "    for word in words:\n",
    "        if word in unique_word_set:\n",
    "            uni_text2+=word+' '\n",
    "    \n",
    "    return fuzz.ratio(uni_text1, uni_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singularize_words(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    singularized_sentence = []\n",
    "\n",
    "    for word in tokens:\n",
    "        # Exclude words that are already singular or not nouns or ending with \"ing\"\n",
    "        if word.lower() in [\"is\", \"am\", \"are\", \"was\", \"were\", \"has\", \"have\", \"had\", \"does\", \"do\", \"did\"]:\n",
    "            singularized_sentence.append(word)\n",
    "        else:\n",
    "            singularized_word = lemmatizer.lemmatize(word)\n",
    "            singularized_sentence.append(singularized_word)\n",
    "    return singularized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text1,text2):\n",
    "    \n",
    "    q1_cc=0\n",
    "    q1_uc =0\n",
    "    q1_ic =0\n",
    "\n",
    "        \n",
    "    q2_cc=0\n",
    "    q2_uc =0\n",
    "    q2_ic =0\n",
    "\n",
    "    q1_common = []\n",
    "    q1_unique = []\n",
    "    q1_intero = []\n",
    "    \n",
    "    \n",
    "    q2_common = []\n",
    "    q2_unique = []\n",
    "    q2_intero = []\n",
    "    \n",
    "    text1 = str(text1)\n",
    "    text2 = str(text2)\n",
    "    # Remove punctuation and split the text into words\n",
    "    \n",
    "    words1 = re.findall(r'\\b\\w+\\b', text1.lower())\n",
    "    words2 = re.findall(r'\\b\\w+\\b', text2.lower())\n",
    "    \n",
    "    str1=\"\"\n",
    "    str2=\"\"\n",
    "    for word in words1:\n",
    "        str1+=word+\" \"\n",
    "        \n",
    "    str1=str1.strip()\n",
    "    \n",
    "    for word in words2:\n",
    "        str2+=word+\" \"\n",
    "        \n",
    "    str2=str2.strip()\n",
    "    \n",
    "    words1 = singularize_words(str1)\n",
    "    words2 = singularize_words(str2)\n",
    "\n",
    "    # print(words1)\n",
    "    # print(words2)\n",
    "    \n",
    "    # Check if the first word is an extra interrogative word\n",
    "    if words1 and words1[0] in extra_intero_words:\n",
    "        q1_intero.append(words1[0])\n",
    "        words1 = words1[1:]\n",
    "    \n",
    "    for word in words1:\n",
    "        if word in common_words:\n",
    "            q1_common.append(word)\n",
    "        elif word in interogative_words:\n",
    "            q1_intero.append(word)\n",
    "        else:\n",
    "            q1_unique.append(word)\n",
    "  \n",
    "  \n",
    "     # Check if the second word is an extra interrogative word\n",
    "    if words2 and words2[0] in extra_intero_words:\n",
    "        q2_intero.append(words2[0])\n",
    "        words2 = words2[1:]\n",
    "    \n",
    "    for word in words2:\n",
    "        if word in common_words:\n",
    "            q2_common.append(word)\n",
    "        elif word in interogative_words:\n",
    "            q2_intero.append(word)\n",
    "        else:\n",
    "            q2_unique.append(word)\n",
    "  \n",
    "    q1_cl = len(q1_common)\n",
    "    q1_ul = len(q1_unique)\n",
    "    q1_il = len(q1_intero)\n",
    "    \n",
    "    q2_cl = len(q2_common)\n",
    "    q2_ul = len(q2_unique)\n",
    "    q2_il = len(q2_intero)\n",
    "    \n",
    "    \n",
    "    \n",
    "    tempc = q2_common.copy()\n",
    "    tempu = q2_unique.copy()\n",
    "    tempi = q2_intero.copy()\n",
    "    \n",
    "    for word in q1_common:\n",
    "        if word in tempc:\n",
    "            q1_cc+=1\n",
    "            tempc.remove(word)\n",
    "    \n",
    "    for word in q1_unique:\n",
    "        if word in tempu:\n",
    "            q1_uc+=1\n",
    "            tempu.remove(word)\n",
    "    \n",
    "    for word in q1_intero:\n",
    "        if word in tempi:\n",
    "            q1_ic+=1\n",
    "            tempi.remove(word)\n",
    "    \n",
    "    \n",
    "    tempc = q1_common.copy()\n",
    "    tempu = q1_unique.copy()\n",
    "    tempi = q1_intero.copy()\n",
    "\n",
    "    for word in q2_common:\n",
    "        if word in tempc:\n",
    "            q2_cc+=1\n",
    "            tempc.remove(word)\n",
    "    \n",
    "    for word in q2_unique:\n",
    "        if word in tempu:\n",
    "            q2_uc+=1\n",
    "            tempu.remove(word)\n",
    "    \n",
    "    for word in q2_intero:\n",
    "        if word in tempi:\n",
    "            q2_ic+=1\n",
    "            tempi.remove(word)\n",
    "            \n",
    "    unique_set = set()\n",
    "    \n",
    "    \n",
    "    for word in q1_unique:\n",
    "        unique_set.add(word)\n",
    "    \n",
    "    for word in q2_unique:\n",
    "        unique_set.add(word)\n",
    "    \n",
    "    fuzzy_ratio=unique_word_out(text1,text2,unique_set)\n",
    "    \n",
    "    if(q1_cl==0):\n",
    "        q1_cl=1\n",
    "    if(q1_ul==0):\n",
    "        q1_ul=1\n",
    "    if(q1_il==0):\n",
    "        q1_il=1\n",
    "    if(q2_cl==0):\n",
    "        q2_cl=1\n",
    "    if(q2_ul==0):\n",
    "        q2_ul=1\n",
    "    if(q2_il==0):\n",
    "        q2_il=1\n",
    "\n",
    "    return q1_cc/q1_cl,q1_uc/q1_ul,q1_ic/q1_il,q2_cc/q2_cl,q2_uc/q2_ul,q2_ic/q2_il,fuzzy_ratio/100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(x_test):\n",
    "    x_test1 = x_test.reshape(1, -1)\n",
    "    print(x_test1)\n",
    "    predictions= model.predict(x_test1)\n",
    "    # Count the occurrences of each number\n",
    "    counter = Counter(predictions)\n",
    "\n",
    "    # Get the number with maximum frequency\n",
    "    max_frequency_number = max(counter, key=counter.get)\n",
    "\n",
    "    return int(max_frequency_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgdg fdg dfgd fgd\n",
      "fg dfg dfg \n",
      "[0.   0.   0.   0.   0.   0.   0.69]\n",
      "[[0.   0.   0.   0.   0.   0.   0.69]]\n",
      "0\n",
      "What is like to have sex with cousin?\n",
      "What is it like to have sex with your cousin?\n",
      "[1.         1.         1.         0.66666667 1.         1.\n",
      " 1.        ]\n",
      "[[1.         1.         1.         0.66666667 1.         1.\n",
      "  1.        ]]\n",
      "1\n",
      "What should I do to be a great geologist?\n",
      "What is it like to have sex with your cousin?\n",
      "[0.16666667 0.         1.         0.16666667 0.         1.\n",
      " 0.38      ]\n",
      "[[0.16666667 0.         1.         0.16666667 0.         1.\n",
      "  0.38      ]]\n",
      "0\n",
      "What should I do to be a great geologist?\n",
      "How can I be a good geologist?\n",
      "[0.5  0.5  0.   0.75 0.5  0.   0.77]\n",
      "[[0.5  0.5  0.   0.75 0.5  0.   0.77]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def concatenate_strings():\n",
    "    ques1 = entry1.get(\"1.0\", \"end-1c\")  # Get text from start to end without the trailing newline character\n",
    "    ques2 = entry2.get(\"1.0\", \"end-1c\")\n",
    "    print(ques1)\n",
    "    print(ques2)\n",
    "    x_test = preprocess_data(ques1, ques2)\n",
    "    x_test = np.array(x_test)\n",
    "    res = get_output(x_test)\n",
    "    print(res)\n",
    "    if(res==1):\n",
    "        ans=\"Duplicate\"\n",
    "    else:\n",
    "        ans=\"Not Duplicate\"\n",
    "    output_label.config(text=\"Question1 and Question2 are \" + ans)\n",
    "\n",
    "# Create main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Quora Question Duplicacy Checker\")\n",
    "\n",
    "# Set window size\n",
    "root.geometry(\"500x400\")\n",
    "\n",
    "label1 = tk.Label(root, text=\"Question 1:\")\n",
    "label1.pack()\n",
    "\n",
    "# Create input text boxes\n",
    "entry1 = tk.Text(root, height=4, width=50)  # Set height to 5 lines\n",
    "entry1.pack(pady=5)\n",
    "\n",
    "# Label for question 2\n",
    "label2 = tk.Label(root, text=\"Question 2:\")\n",
    "label2.pack()\n",
    "\n",
    "entry2 = tk.Text(root, height=4, width=50)  # Set height to 5 lines\n",
    "entry2.pack(pady=5)\n",
    "\n",
    "# Create button\n",
    "check_button = tk.Button(root, text=\"Check\", command=concatenate_strings)\n",
    "check_button.pack(pady=5)\n",
    "\n",
    "# Create output label\n",
    "output_label = tk.Label(root, text=\"\")\n",
    "output_label.pack(pady=5)\n",
    "\n",
    "# Run the main event loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
